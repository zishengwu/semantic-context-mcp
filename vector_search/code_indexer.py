from pathlib import Path
from datetime import datetime
from typing import List, Dict
from hashlib import md5
import os
from openai import OpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter

from .vector_db import VectorDBManager
from .ast_parser import SmartASTParser
from .code_change_tracker import CodeChangeTracker

class IncrementalCodeIndexer:
    """å¢é‡ä»£ç ç´¢å¼•ç³»ç»Ÿ

    æä¾›ï¼š
    - run_incremental_indexing() : ä»…å¤„ç†å˜æ›´çš„æ–‡ä»¶
    - full_index() : é¦–æ¬¡æˆ–å¼ºåˆ¶å…¨é‡ç´¢å¼•
    """

    def __init__(self, embedding_client:OpenAI, vector_db_manager: VectorDBManager):
        self.parser = SmartASTParser()
        self.vector_db = vector_db_manager
        self.embedding_client = embedding_client

    def run_incremental_indexing(self, project_root_str: str):
        """æ‰§è¡Œå¢é‡ç´¢å¼•"""
        print("ğŸ” æ£€æµ‹ä»£ç å˜æ›´...")
        changes = CodeChangeTracker.detect_changes(project_root_str)

        print(f"ğŸ“Š å˜æ›´ç»Ÿè®¡:")
        print(f"æ–°å¢æ–‡ä»¶: {len(changes['added'])}")
        print(f"ä¿®æ”¹æ–‡ä»¶: {len(changes['modified'])}")
        print(f"åˆ é™¤æ–‡ä»¶: {len(changes['deleted'])}")
        print(f"æœªå˜æ›´æ–‡ä»¶: {len(changes['unchanged'])}")

        project_root = Path(project_root_str)
        proj_name = project_root.name
        proj_hash = md5(str(project_root.resolve()).encode()).hexdigest()[:8]
        collection_name = f"{proj_name}-{proj_hash}"

        # å¤„ç†åˆ é™¤çš„æ–‡ä»¶
        if changes['deleted']:
            print("ğŸ—‘ï¸  å¤„ç†åˆ é™¤çš„æ–‡ä»¶...")
            for file_path in changes['deleted']:
                self.vector_db.delete_blocks_by_file(collection_name, file_path)
            CodeChangeTracker.remove_file_hash(project_root_str, changes['deleted'])

        # å¤„ç†æ–°å¢å’Œä¿®æ”¹çš„æ–‡ä»¶
        files_to_process = changes['added'] + changes['modified']
        if files_to_process:
            print(f"ğŸ”„ å¤„ç† {len(files_to_process)} ä¸ªæ–‡ä»¶...")
            self._process_files(project_root_str, files_to_process)

        # æ›´æ–°å…ƒæ•°æ®
        metadata = CodeChangeTracker.load_metadata(project_root_str)
        metadata['last_index_time'] = datetime.now().isoformat()
        file_hashes = CodeChangeTracker._collect_file_hashes(project_root_str)
        metadata['total_files_indexed'] = len(file_hashes)
        CodeChangeTracker.save_metadata(project_root_str, metadata)

        print("âœ… å¢é‡ç´¢å¼•å®Œæˆï¼")

    def full_index(self, project_root_str: str):
        """æ‰§è¡Œå®Œæ•´ç´¢å¼•ï¼šæ‰«ææ‰€æœ‰å¯ç´¢å¼•çš„ .py æ–‡ä»¶å¹¶ä½œä¸ºæ–°å¢å¤„ç†"""
        all_py_files = []
        EXTS = {".py", ".java", ".cpp", ".cc", ".cxx", ".c", ".js", ".jsx", ".mjs", ".ts", ".tsx", ".go"}
        project_path = Path(project_root_str)
        for file_path in project_path.rglob('*'):
            if file_path.suffix.lower() in EXTS:
                if CodeChangeTracker._should_index_file(file_path):
                    rel_path = str(file_path.relative_to(project_path))
                    all_py_files.append(rel_path)

        if not all_py_files:
            print("No python files found to index.")
            return

        # treat everything as added for first-time index
        print(f"ğŸ” å…¨é‡ç´¢å¼•ï¼šå‘ç° {len(all_py_files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
        self._process_files(project_root_str, all_py_files)
        # update metadata
        metadata = CodeChangeTracker.load_metadata(project_root_str)
        metadata['last_index_time'] = datetime.now().isoformat()
        file_hashes = CodeChangeTracker._collect_file_hashes(project_root_str)
        metadata['total_files_indexed'] = len(file_hashes)
        CodeChangeTracker.save_metadata(project_root_str, metadata)

    def _process_files(self, project_root_str:str, file_paths: List[str]):
        """å¤„ç†æ–‡ä»¶åˆ—è¡¨"""
        new_hashes = []
        all_blocks = []

        project_root = Path(project_root_str)
        proj_name = project_root.name
        proj_hash = md5(str(project_root.resolve()).encode()).hexdigest()[:8]
        collection_name = f"{proj_name}-{proj_hash}"

        file_hashes = CodeChangeTracker._collect_file_hashes(project_root_str)
        for rel_path in file_paths:
            file_path = Path(project_root_str) / rel_path

            # åˆ é™¤æ—§ç´¢å¼•ï¼ˆå¯¹äºä¿®æ”¹çš„æ–‡ä»¶ï¼‰
            if rel_path in file_hashes:
                self.vector_db.delete_blocks_by_file(collection_name, str(file_path))

            # è§£ææ–°ä»£ç å—
            blocks = self.parser.extract_code_blocks(file_path)
            all_blocks.extend(blocks)

            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
            file_hash = CodeChangeTracker.compute_file_hash(file_path)
            new_hashes.append((rel_path, file_hash))

            print(f"  å¤„ç† {rel_path}: æ‰¾åˆ° {len(blocks)} ä¸ªä»£ç å—")

        # ç”ŸæˆåµŒå…¥å‘é‡
        if all_blocks:
            print("ğŸ§  ç”ŸæˆåµŒå…¥å‘é‡...")
            
            # å¤„ç†é•¿æ–‡æœ¬å—çš„åˆ‡åˆ†
            all_processed_blocks = []
            all_processed_texts = []
            
            for block in all_blocks:
                text = self._prepare_text_for_embedding(block)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡åˆ†
                max_length = os.environ.get("MAX_LENGTH", 8000)
                if len(text) <= max_length:
                    all_processed_blocks.append(block)
                    all_processed_texts.append(text)
                else:
                        
                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=int(os.environ.get("CHUNK_SIZE", 4000)),
                        chunk_overlap=int(os.environ.get("CHUNK_OVERLAP", 200)),
                        length_function=len,
                        separators=["\n\n", "\n", " ", ""]
                    )
                    chunks = text_splitter.split_text(text)
                    
                    # ä¸ºæ¯ä¸ªchunkåˆ›å»ºæ–°çš„block
                    for i, chunk in enumerate(chunks):
                        new_block = block.copy()
                        new_block['name'] = f"{block['name']}_chunk_{i+1}"
                        new_block['code'] = chunk  # ä½¿ç”¨åˆ‡åˆ†åçš„ä»£ç 
                        new_block['signature'] = f"{block['signature']} (part {i+1})"
                        # ä¸ºåˆ‡åˆ†åçš„å—ç”Ÿæˆå”¯ä¸€ID
                        new_block['id'] = f"{block['id']}_chunk_{i+1}"
                        all_processed_blocks.append(new_block)
                        all_processed_texts.append(chunk)

            print(f"å¤„ç†åçš„æ–‡æœ¬å—: {len(all_processed_texts)}")
            
            embeddings = []
            valid_blocks = []
            
            for i, (text, block) in enumerate(zip(all_processed_texts, all_processed_blocks)):
                try:
                    response = self.embedding_client.embeddings.create(input=[text], model="jina")
                    embeddings.append(response.data[0].embedding)
                    valid_blocks.append(block)
                except Exception as e:
                    print(f"å¤„ç†æ–‡æœ¬å— {i} æ—¶å‡ºé”™: {e}")
                    print(f"æ–‡æœ¬é•¿åº¦: {len(text)}")
                    continue
            
            # æ›´æ–°å‘é‡æ•°æ®åº“
            print("ğŸ’¾ æ›´æ–°å‘é‡æ•°æ®åº“...")
            if embeddings and valid_blocks:
                self.vector_db.upsert_blocks(collection_name, valid_blocks, embeddings)
            else:
                print("âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„åµŒå…¥å‘é‡æˆ–ä»£ç å—éœ€è¦æ›´æ–°")

        # æ›´æ–°æ–‡ä»¶å“ˆå¸Œè®°å½•
        file_paths, hashes = zip(*new_hashes) if new_hashes else ([], [])
        CodeChangeTracker.update_file_hashes(project_root_str, list(file_paths), list(hashes))

    def _prepare_text_for_embedding(self, block: Dict) -> str:
        """å‡†å¤‡ç”¨äºåµŒå…¥çš„æ–‡æœ¬"""
        # ç»„åˆä»£ç ã€ç­¾åå’Œä¸Šä¸‹æ–‡ä¿¡æ¯
        parts = [
            f"Type: {block['type']}",
            f"Name: {block['name']}",
            f"Signature: {block['signature']}",
            f"Code: {block['code']}"
        ]
        return "\n".join(parts)

    def get_index_status(self, project_root_str: str):
        """è·å–ç´¢å¼•çŠ¶æ€"""
        metadata = CodeChangeTracker.load_metadata(project_root_str)
        file_hashes = CodeChangeTracker._collect_file_hashes(project_root_str)
        return {
            'last_index_time': metadata['last_index_time'],
            'total_files': len(file_hashes),
            'file_hashes': file_hashes,
            'path': str(Path(project_root_str) / ".code_index")
        }